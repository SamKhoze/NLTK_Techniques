{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_Cheatsheet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "hkjIjttZ5dPy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAAWH7PU5bU6",
        "outputId": "b2b0f530-efaf-4678-eeda-dde6d336f5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "import nltk.corpus"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(nltk.data.find(\"corpora\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa6VhP6F52Ad",
        "outputId": "8a0173fb-05f9-4c73-f23e-a93f3b6776bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['toolbox', 'verbnet3', 'unicode_samples', 'subjectivity', 'reuters.zip', 'problem_reports', 'senseval.zip', 'gazetteers.zip', 'masc_tagged.zip', 'pl196x', 'timit.zip', 'europarl_raw', 'lin_thesaurus', 'names.zip', 'rte.zip', 'qc.zip', 'city_database', 'panlex_swadesh.zip', 'qc', 'cess_esp.zip', 'udhr.zip', 'problem_reports.zip', 'mac_morpho', 'dolch.zip', 'ppattach.zip', 'comparative_sentences', 'cess_cat', 'framenet_v15', 'universal_treebanks_v20.zip', 'udhr', 'words.zip', 'ieer.zip', 'indian', 'crubadan', 'kimmo', 'subjectivity.zip', 'opinion_lexicon.zip', 'indian.zip', 'rte', 'names', 'pil.zip', 'machado.zip', 'senseval', 'swadesh.zip', 'conll2000.zip', 'omw-1.4.zip', 'webtext.zip', 'udhr2', 'unicode_samples.zip', 'switchboard', 'timit', 'shakespeare.zip', 'swadesh', 'sentence_polarity.zip', 'nonbreaking_prefixes.zip', 'ptb.zip', 'nombank.1.0.zip', 'extended_omw.zip', 'abc.zip', 'wordnet2021.zip', 'knbc.zip', 'pe08.zip', 'paradigms.zip', 'chat80', 'city_database.zip', 'verbnet3.zip', 'ycoe.zip', 'lin_thesaurus.zip', 'paradigms', 'propbank.zip', 'verbnet.zip', 'smultron.zip', 'opinion_lexicon', 'omw.zip', 'dolch', 'sentiwordnet.zip', 'treebank.zip', 'shakespeare', 'abc', 'product_reviews_2', 'genesis.zip', 'pe08', 'biocreative_ppi.zip', 'biocreative_ppi', 'stopwords', 'conll2002.zip', 'crubadan.zip', 'pl196x.zip', 'dependency_treebank', 'mac_morpho.zip', 'kimmo.zip', 'wordnet_ic', 'gazetteers', 'framenet_v15.zip', 'state_union.zip', 'cmudict.zip', 'treebank', 'wordnet_ic.zip', 'pros_cons.zip', 'ycoe', 'cess_cat.zip', 'conll2002', 'words', 'gutenberg', 'floresta', 'brown.zip', 'jeita.zip', 'twitter_samples.zip', 'mte_teip5.zip', 'sinica_treebank.zip', 'comparative_sentences.zip', 'comtrans.zip', 'brown', 'genesis', 'movie_reviews.zip', 'wordnet31.zip', 'switchboard.zip', 'verbnet', 'udhr2.zip', 'dependency_treebank.zip', 'nonbreaking_prefixes', 'ptb', 'cess_esp', 'pil', 'cmudict', 'webtext', 'gutenberg.zip', 'conll2007.zip', 'semcor.zip', 'product_reviews_2.zip', 'mte_teip5', 'conll2000', 'product_reviews_1', 'brown_tei', 'ieer', 'alpino', 'nps_chat', 'pros_cons', 'twitter_samples', 'nps_chat.zip', 'alpino.zip', 'sentiwordnet', 'toolbox.zip', 'sentence_polarity', 'state_union', 'ppattach', 'smultron', 'inaugural.zip', 'wordnet.zip', 'movie_reviews', 'inaugural', 'floresta.zip', 'framenet_v17', 'sinica_treebank', 'europarl_raw.zip', 'chat80.zip', 'brown_tei.zip', 'stopwords.zip', 'product_reviews_1.zip', 'framenet_v17.zip']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9CiwhMy7wa4",
        "outputId": "30d31c4a-2c72-43e1-d5c4-67d52150bef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbMMWael7K2a",
        "outputId": "3f2e9604-bc45-4446-d367-6c9da1154f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet = nltk.corpus.gutenberg.words (\"shakespeare-hamlet.txt\")\n",
        "print(hamlet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFLVnrhZ7cMG",
        "outputId": "6f2899d3-dbf7-4cdc-858e-8abf733100f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 500 words"
      ],
      "metadata": {
        "id": "6CO9jMxv8yON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in hamlet[:500]:\n",
        "  print(word, sep= '', end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1y7Zg482w9",
        "outputId": "498a7d35-b47e-4b96-cefb-cf76b16426ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TheTragedieofHamletbyWilliamShakespeare1599]ActusPrimus.ScoenaPrima.EnterBarnardoandFranciscotwoCentinels.Barnardo.Who'sthere?Fran.Nayanswerme:Stand&vnfoldyourselfeBar.LongliuetheKingFran.Barnardo?Bar.HeFran.YoucomemostcarefullyvponyourhoureBar.'Tisnowstrooktwelue,gettheetobedFranciscoFran.Forthisreleefemuchthankes:'Tisbittercold,AndIamsickeatheartBarn.HaueyouhadquietGuard?Fran.NotaMousestirringBarn.Well,goodnight.IfyoudomeetHoratioandMarcellus,theRiualsofmyWatch,bidthemmakehast.EnterHoratioandMarcellus.Fran.IthinkeIhearethem.Stand:who'sthere?Hor.FriendstothisgroundMar.AndLeige-mentotheDaneFran.GiueyougoodnightMar.OfarwelhonestSoldier,whohathrelieu'dyou?Fra.Barnardoha'smyplace:giueyougoodnight.ExitFran.Mar.HollaBarnardoBar.Say,whatisHoratiothere?Hor.ApeeceofhimBar.WelcomeHoratio,welcomegoodMarcellusMar.What,ha'sthisthingappear'dagainetonightBar.IhaueseenenothingMar.Horatiosaies,'tisbutourFantasie,AndwillnotletbeleefetakeholdofhimTouchingthisdreadedsight,twiceseeneofvs,ThereforeIhaueintreatedhimalongWithvs,towatchtheminutesofthisNight,ThatifagainethisApparitioncome,Hemayapproueoureyes,andspeaketoitHor.Tush,tush,'twillnotappeareBar.Sitdownea-while,Andletvsonceagaineassaileyoureares,ThataresofortifiedagainstourStory,WhatwetwoNightshaueseeneHor.Well,sitwedowne,AndletvsheareBarnardospeakeofthisBarn.Lastnightofall,WhenyondsameStarrethat'sWestwardfromthePoleHadmadehiscourset'illumethatpartofHeauenWherenowitburnes,Marcellusandmyselfe,TheBellthenbeatingoneMar.Peace,breaketheeof:EntertheGhost.LookewhereitcomesagaineBarn.Inthesamefigure,liketheKingthat'sdeadMar.ThouartaScholler;speaketoitHoratioBarn.LookesitnotliketheKing?MarkeitHoratioHora.Mostlike:Itharrowesmewithfear&wonderBarn.ItwouldbespoketooMar.QuestionitHoratioHor.Whatart"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create some text"
      ],
      "metadata": {
        "id": "6Wnc1VwW93am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AI = \"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs.” Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think. While exploiting the power of the computer systems, the curiosity of human, lead him to wonder, “Can a machine think and behave like humans do?”\""
      ],
      "metadata": {
        "id": "sZoKt2LE93C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(AI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFVZKOg69Z3d",
        "outputId": "54a22a33-0e84-4716-eefd-83e63a3749d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the text"
      ],
      "metadata": {
        "id": "bdrhGL7J_nFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "nTvAVERj_AX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AI_tokens = word_tokenize(AI)\n",
        "AI_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU_cOJsb_LQS",
        "outputId": "dc4a66f3-2e22-404d-b380-2c945cec17e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['According',\n",
              " 'to',\n",
              " 'the',\n",
              " 'father',\n",
              " 'of',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'John',\n",
              " 'McCarthy',\n",
              " ',',\n",
              " 'it',\n",
              " 'is',\n",
              " '“',\n",
              " 'The',\n",
              " 'science',\n",
              " 'and',\n",
              " 'engineering',\n",
              " 'of',\n",
              " 'making',\n",
              " 'intelligent',\n",
              " 'machines',\n",
              " ',',\n",
              " 'especially',\n",
              " 'intelligent',\n",
              " 'computer',\n",
              " 'programs.',\n",
              " '”',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'is',\n",
              " 'a',\n",
              " 'way',\n",
              " 'of',\n",
              " 'making',\n",
              " 'a',\n",
              " 'computer',\n",
              " ',',\n",
              " 'a',\n",
              " 'computer-controlled',\n",
              " 'robot',\n",
              " ',',\n",
              " 'or',\n",
              " 'a',\n",
              " 'software',\n",
              " 'think',\n",
              " 'intelligently',\n",
              " ',',\n",
              " 'in',\n",
              " 'the',\n",
              " 'similar',\n",
              " 'manner',\n",
              " 'the',\n",
              " 'intelligent',\n",
              " 'humans',\n",
              " 'think',\n",
              " '.',\n",
              " 'While',\n",
              " 'exploiting',\n",
              " 'the',\n",
              " 'power',\n",
              " 'of',\n",
              " 'the',\n",
              " 'computer',\n",
              " 'systems',\n",
              " ',',\n",
              " 'the',\n",
              " 'curiosity',\n",
              " 'of',\n",
              " 'human',\n",
              " ',',\n",
              " 'lead',\n",
              " 'him',\n",
              " 'to',\n",
              " 'wonder',\n",
              " ',',\n",
              " '“',\n",
              " 'Can',\n",
              " 'a',\n",
              " 'machine',\n",
              " 'think',\n",
              " 'and',\n",
              " 'behave',\n",
              " 'like',\n",
              " 'humans',\n",
              " 'do',\n",
              " '?',\n",
              " '”']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the number of token"
      ],
      "metadata": {
        "id": "ClS7zcir_yOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(AI_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5mm543G_1uL",
        "outputId": "aae989d0-1c78-4532-9ec8-79e13da83445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting report of the word count in the text"
      ],
      "metadata": {
        "id": "HvqrHs0_Ar9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()"
      ],
      "metadata": {
        "id": "s56SrKi3AEYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in AI_tokens:\n",
        "  fdist[word.lower()]+=2\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8f6QNrYARjX",
        "outputId": "69b23366-ee62-4106-cc20-c94ccce26e17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 18,\n",
              "          '.': 2,\n",
              "          '?': 2,\n",
              "          'a': 10,\n",
              "          'according': 2,\n",
              "          'and': 4,\n",
              "          'artificial': 4,\n",
              "          'behave': 2,\n",
              "          'can': 2,\n",
              "          'computer': 6,\n",
              "          'computer-controlled': 2,\n",
              "          'curiosity': 2,\n",
              "          'do': 2,\n",
              "          'engineering': 2,\n",
              "          'especially': 2,\n",
              "          'exploiting': 2,\n",
              "          'father': 2,\n",
              "          'him': 2,\n",
              "          'human': 2,\n",
              "          'humans': 4,\n",
              "          'in': 2,\n",
              "          'intelligence': 4,\n",
              "          'intelligent': 6,\n",
              "          'intelligently': 2,\n",
              "          'is': 4,\n",
              "          'it': 2,\n",
              "          'john': 2,\n",
              "          'lead': 2,\n",
              "          'like': 2,\n",
              "          'machine': 2,\n",
              "          'machines': 2,\n",
              "          'making': 4,\n",
              "          'manner': 2,\n",
              "          'mccarthy': 2,\n",
              "          'of': 10,\n",
              "          'or': 2,\n",
              "          'power': 2,\n",
              "          'programs.': 2,\n",
              "          'robot': 2,\n",
              "          'science': 2,\n",
              "          'similar': 2,\n",
              "          'software': 2,\n",
              "          'systems': 2,\n",
              "          'the': 14,\n",
              "          'think': 6,\n",
              "          'to': 4,\n",
              "          'way': 2,\n",
              "          'while': 2,\n",
              "          'wonder': 2,\n",
              "          '“': 4,\n",
              "          '”': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting a report of a specific word how many times repated in the text"
      ],
      "metadata": {
        "id": "XYFqbrQmA2Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist['artificial']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFaZO0CLA-Aw",
        "outputId": "2fe2143a-a89b-49ff-fab3-75df1aba3b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting number of disting tokens"
      ],
      "metadata": {
        "id": "jz8uCMLeBmx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CUKHe-jBsfB",
        "outputId": "3291178e-562f-479f-eef9-10893d1ae0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the most common words (in this case top 10)"
      ],
      "metadata": {
        "id": "0Sf9hYdRB43B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg9gzCzLB4g4",
        "outputId": "68097f09-8d5b-410d-ea05-93ecbcbde7aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 18),\n",
              " ('the', 14),\n",
              " ('of', 10),\n",
              " ('a', 10),\n",
              " ('intelligent', 6),\n",
              " ('computer', 6),\n",
              " ('think', 6),\n",
              " ('to', 4),\n",
              " ('artificial', 4),\n",
              " ('intelligence', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the number of paragraph "
      ],
      "metadata": {
        "id": "A6KJPd8bCPlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_blank = blankline_tokenize(AI)\n",
        "len(AI_blank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGAT3BbmCb11",
        "outputId": "041954a3-cea2-4836-e96e-bd3685acf7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see an specific pragraph (in this case paragraph number 1)"
      ],
      "metadata": {
        "id": "9JkkOQedCzWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AI_blank[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "flKrAawCC86O",
        "outputId": "bc9d2959-a332-4fca-c992-48aae7aace67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs.” Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think. While exploiting the power of the computer systems, the curiosity of human, lead him to wonder, “Can a machine think and behave like humans do?”'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ngrams** \n",
        "## are tokens of any number of consecutive written words \n",
        "# **Bigrams** \n",
        "## are tokens of two consecutive written words\n",
        "# **Trigrams** \n",
        "## are tokens of three consecutive written words"
      ],
      "metadata": {
        "id": "w1kXS6pvDPCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "cDsRXtzyEBmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams"
      ],
      "metadata": {
        "id": "FsikbWe-D-oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"the best and most beautiful things in the world can not be seen or even touched, they must be felt with the heart\"\n",
        "quotes_tokens = nltk.word_tokenize(string)\n",
        "quotes_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET-jatx3EOo6",
        "outputId": "588b2209-4d2a-4c2e-d3af-a89ba3ac407d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'best',\n",
              " 'and',\n",
              " 'most',\n",
              " 'beautiful',\n",
              " 'things',\n",
              " 'in',\n",
              " 'the',\n",
              " 'world',\n",
              " 'can',\n",
              " 'not',\n",
              " 'be',\n",
              " 'seen',\n",
              " 'or',\n",
              " 'even',\n",
              " 'touched',\n",
              " ',',\n",
              " 'they',\n",
              " 'must',\n",
              " 'be',\n",
              " 'felt',\n",
              " 'with',\n",
              " 'the',\n",
              " 'heart']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a bigrams"
      ],
      "metadata": {
        "id": "gS4JVah1GIHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_bigrams = list (nltk.bigrams(quotes_tokens))\n",
        "quotes_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLqdFybrFaWF",
        "outputId": "4c9e8e6e-bd4a-4a16-ceac-d056dc021ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'best'),\n",
              " ('best', 'and'),\n",
              " ('and', 'most'),\n",
              " ('most', 'beautiful'),\n",
              " ('beautiful', 'things'),\n",
              " ('things', 'in'),\n",
              " ('in', 'the'),\n",
              " ('the', 'world'),\n",
              " ('world', 'can'),\n",
              " ('can', 'not'),\n",
              " ('not', 'be'),\n",
              " ('be', 'seen'),\n",
              " ('seen', 'or'),\n",
              " ('or', 'even'),\n",
              " ('even', 'touched'),\n",
              " ('touched', ','),\n",
              " (',', 'they'),\n",
              " ('they', 'must'),\n",
              " ('must', 'be'),\n",
              " ('be', 'felt'),\n",
              " ('felt', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create trigrams"
      ],
      "metadata": {
        "id": "GOTSW5aZGZ1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_trigrams = list (nltk.trigrams(quotes_tokens))\n",
        "quotes_trigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ULJrrI-GNfS",
        "outputId": "5e3219a3-d9fd-4282-84fe-1b06ff1af339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'best', 'and'),\n",
              " ('best', 'and', 'most'),\n",
              " ('and', 'most', 'beautiful'),\n",
              " ('most', 'beautiful', 'things'),\n",
              " ('beautiful', 'things', 'in'),\n",
              " ('things', 'in', 'the'),\n",
              " ('in', 'the', 'world'),\n",
              " ('the', 'world', 'can'),\n",
              " ('world', 'can', 'not'),\n",
              " ('can', 'not', 'be'),\n",
              " ('not', 'be', 'seen'),\n",
              " ('be', 'seen', 'or'),\n",
              " ('seen', 'or', 'even'),\n",
              " ('or', 'even', 'touched'),\n",
              " ('even', 'touched', ','),\n",
              " ('touched', ',', 'they'),\n",
              " (',', 'they', 'must'),\n",
              " ('they', 'must', 'be'),\n",
              " ('must', 'be', 'felt'),\n",
              " ('be', 'felt', 'with'),\n",
              " ('felt', 'with', 'the'),\n",
              " ('with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create ngrams"
      ],
      "metadata": {
        "id": "iFb5_fLTGdz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_ngrams = list (nltk.ngrams(quotes_tokens, 5))\n",
        "quotes_ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHW9hmdXGg2d",
        "outputId": "3f8ebcd8-9ce4-4d94-b0e2-2e505b8e3aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'best', 'and', 'most', 'beautiful'),\n",
              " ('best', 'and', 'most', 'beautiful', 'things'),\n",
              " ('and', 'most', 'beautiful', 'things', 'in'),\n",
              " ('most', 'beautiful', 'things', 'in', 'the'),\n",
              " ('beautiful', 'things', 'in', 'the', 'world'),\n",
              " ('things', 'in', 'the', 'world', 'can'),\n",
              " ('in', 'the', 'world', 'can', 'not'),\n",
              " ('the', 'world', 'can', 'not', 'be'),\n",
              " ('world', 'can', 'not', 'be', 'seen'),\n",
              " ('can', 'not', 'be', 'seen', 'or'),\n",
              " ('not', 'be', 'seen', 'or', 'even'),\n",
              " ('be', 'seen', 'or', 'even', 'touched'),\n",
              " ('seen', 'or', 'even', 'touched', ','),\n",
              " ('or', 'even', 'touched', ',', 'they'),\n",
              " ('even', 'touched', ',', 'they', 'must'),\n",
              " ('touched', ',', 'they', 'must', 'be'),\n",
              " (',', 'they', 'must', 'be', 'felt'),\n",
              " ('they', 'must', 'be', 'felt', 'with'),\n",
              " ('must', 'be', 'felt', 'with', 'the'),\n",
              " ('be', 'felt', 'with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming**\n",
        "## Normalize words into its base from or roof form\n",
        "\n",
        "for example: Affection, Affects, Affections, Affected, Affectiong their root is Affect"
      ],
      "metadata": {
        "id": "fF7p61XPG0za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()"
      ],
      "metadata": {
        "id": "QGMpkL0nHBfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test for the word \"Having\""
      ],
      "metadata": {
        "id": "nCjWlzazH5kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pst.stem(\"having\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSNFJNofH-JR",
        "outputId": "8c785b26-e8ac-4415-e3b4-982f23b28248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words to stem"
      ],
      "metadata": {
        "id": "GduKX3u_IQrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_stem = [\"give\", \"giving\", \"given\", \"gave\"]\n",
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +pst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EDEza9uIMKf",
        "outputId": "21dc177b-9d0b-453b-b6ee-261d9f93e1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming with Lancaster"
      ],
      "metadata": {
        "id": "DJAfRo1ZJB15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +lst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdqIBfCUJF-k",
        "outputId": "f7114708-5aa0-48d1-e932-b41705b8ec8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:giv\n",
            "giving:giv\n",
            "given:giv\n",
            "gave:gav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming with Snowball"
      ],
      "metadata": {
        "id": "WUAIQLZiKAvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "NkiUZaaYJz2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +sbst.stem(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArcvWz30KEcs",
        "outputId": "bf2f63d6-b616-49dd-e6eb-28c9c04c9fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**\n",
        "\n",
        "Groups together diffrent inflected forms of a word, called Lemma.\n",
        "\n",
        "It is Similar to Stemming somehoew, as it maps several words into one common root.\n",
        "\n",
        "Output of lemmatisation is a proper word.\n",
        "\n",
        "For example a Lemmatiser should map **gone**, **going** and **went** into ***go***"
      ],
      "metadata": {
        "id": "0Q18RHyDKlT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jfiO65lX54c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_lem.lemmatize(\"corpora\")"
      ],
      "metadata": {
        "id": "VBEORiCz6V4z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "288ec44f-4aff-4912-b58e-175251adee21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words in words_to_stem:\n",
        "  print(words+ \":\" +word_lem.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG785xQ77aK3",
        "outputId": "1dcbbd41-d496-49e9-f760-8f8693c3dc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Words"
      ],
      "metadata": {
        "id": "hu7BsWEz-RCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ZphObS1t-QO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yBs_Asx-MKL",
        "outputId": "2cef4605-4892-4650-f95e-0ebbc99288df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## look at the numbers of stopwords we have "
      ],
      "metadata": {
        "id": "LlIy3hJU-2i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx7jmpPm-zpp",
        "outputId": "ed8bbe55-430c-4889-de87-b3414ccad5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUNKNtJ0_F41",
        "outputId": "3cc3d3c8-102f-4360-9d9d-040dfc17cdd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 18),\n",
              " ('the', 14),\n",
              " ('of', 10),\n",
              " ('a', 10),\n",
              " ('intelligent', 6),\n",
              " ('computer', 6),\n",
              " ('think', 6),\n",
              " ('to', 4),\n",
              " ('artificial', 4),\n",
              " ('intelligence', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
      ],
      "metadata": {
        "id": "VPLWYU5S_NEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuation = []\n",
        "for words in AI_tokens:\n",
        "  word = punctuation.sub(\"\", words)\n",
        "  if len(word)>0:\n",
        "    post_punctuation.append(word)"
      ],
      "metadata": {
        "id": "gn9NLdCe_zHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8PoktEqATEH",
        "outputId": "7f119555-5a6a-4ee1-cabc-7f7880e9b41d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['According',\n",
              " 'to',\n",
              " 'the',\n",
              " 'father',\n",
              " 'of',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'John',\n",
              " 'McCarthy',\n",
              " 'it',\n",
              " 'is',\n",
              " '“',\n",
              " 'The',\n",
              " 'science',\n",
              " 'and',\n",
              " 'engineering',\n",
              " 'of',\n",
              " 'making',\n",
              " 'intelligent',\n",
              " 'machines',\n",
              " 'especially',\n",
              " 'intelligent',\n",
              " 'computer',\n",
              " 'programs',\n",
              " '”',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " 'is',\n",
              " 'a',\n",
              " 'way',\n",
              " 'of',\n",
              " 'making',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'a',\n",
              " 'computercontrolled',\n",
              " 'robot',\n",
              " 'or',\n",
              " 'a',\n",
              " 'software',\n",
              " 'think',\n",
              " 'intelligently',\n",
              " 'in',\n",
              " 'the',\n",
              " 'similar',\n",
              " 'manner',\n",
              " 'the',\n",
              " 'intelligent',\n",
              " 'humans',\n",
              " 'think',\n",
              " 'While',\n",
              " 'exploiting',\n",
              " 'the',\n",
              " 'power',\n",
              " 'of',\n",
              " 'the',\n",
              " 'computer',\n",
              " 'systems',\n",
              " 'the',\n",
              " 'curiosity',\n",
              " 'of',\n",
              " 'human',\n",
              " 'lead',\n",
              " 'him',\n",
              " 'to',\n",
              " 'wonder',\n",
              " '“',\n",
              " 'Can',\n",
              " 'a',\n",
              " 'machine',\n",
              " 'think',\n",
              " 'and',\n",
              " 'behave',\n",
              " 'like',\n",
              " 'humans',\n",
              " 'do',\n",
              " '”']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS & Tagging"
      ],
      "metadata": {
        "id": "SlLlnNaeA6B2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"Coco is a natural when it comes to drawing\"\n",
        "sent_tokens = word_tokenize (sent)"
      ],
      "metadata": {
        "id": "xxPPVPGrA5bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffGP0MK-BQgO",
        "outputId": "8291a35b-9c5b-417f-da54-8ebeddb3da70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Coco', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('drawing', 'VBG')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "another example"
      ],
      "metadata": {
        "id": "l1L-QPjLBvQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent2 = \"Sam is eating a delicious cake\"\n",
        "sent2_tokens = word_tokenize(sent2)\n",
        "for token in sent2_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecRQy82DBrB4",
        "outputId": "5174ca3b-86b1-4dc0-b22e-b9c122ec840b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sam', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('eating', 'VBG')]\n",
            "[('a', 'DT')]\n",
            "[('delicious', 'JJ')]\n",
            "[('cake', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition\n",
        "\n",
        "such as Movie, Monetary Value, Organization, Location, Quantities, Person, etc..."
      ],
      "metadata": {
        "id": "5Shr0L2eCbBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk"
      ],
      "metadata": {
        "id": "OqeqBmXFCYMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_sent =\"The US President stays in the WHITE HOUSE\""
      ],
      "metadata": {
        "id": "1c9M0dCfJu9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_tokens = word_tokenize(NE_sent)\n",
        "NE_tags = nltk.pos_tag(NE_tokens)"
      ],
      "metadata": {
        "id": "4S_cQeCdJ7Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUIWNebcKXSW",
        "outputId": "572cfce0-6ee7-4fd7-8eb9-b5ac866e7b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  President/NNP\n",
            "  stays/VBZ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Syntax\n",
        "Syntax Tree is a tree representation of syntatic structure of sentences or strings"
      ],
      "metadata": {
        "id": "5mUcp-xBLF2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = \"The big cat ate little mouse who was after fresh cheese\"\n",
        "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L09Os0V8KkYd",
        "outputId": "025bdfbf-c7a4-435a-d0fb-18fb9545be56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('big', 'JJ'),\n",
              " ('cat', 'NN'),\n",
              " ('ate', 'NN'),\n",
              " ('little', 'JJ'),\n",
              " ('mouse', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('was', 'VBD'),\n",
              " ('after', 'IN'),\n",
              " ('fresh', 'JJ'),\n",
              " ('cheese', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammar_np = r\"NP: {<DT>?<JJ>*<NN>}\""
      ],
      "metadata": {
        "id": "4g5OoR0rRRJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammar_np)"
      ],
      "metadata": {
        "id": "xw5kcXavSIz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "chunk_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "CvLLDmeGSfHO",
        "outputId": "589d3092-3afd-483e-fdac-ea9a20e8590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msvgling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdraw_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repr_svg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svgling'"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), Tree('NP', [('ate', 'NN')]), Tree('NP', [('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    }
  ]
}